<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Success & Failures - Door Detection Tutorial</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <h1 class="nav-title">Door Detection Tutorial</h1>
            <ul class="nav-menu">
                <li><a href="index.html">Home</a></li>
                <li><a href="problem.html">Problem & Users</a></li>
                <li><a href="sensors.html">Sensors</a></li>
                <li><a href="classical.html">Classical Methods</a></li>
                <li><a href="learning.html">Learning Methods</a></li>
                <li><a href="evaluation.html" class="active">Success & Failures</a></li>
                <li><a href="challenges.html">Challenges</a></li>
                <li><a href="future.html">Future</a></li>
                <li><a href="quiz.html">Interactive Quiz</a></li>
                <li><a href="bibliography.html">Bibliography</a></li>
            </ul>
        </div>
    </nav>

    <main class="container">
        <section class="content-section">
            <div class="audio-player">
                <p>üîä <strong>Audio Narration:</strong> Listen to this section</p>
                <audio controls>
                    <source src="audio/evaluation.mp3" type="audio/mpeg">
                    Your browser does not support the audio element.
                </audio>
                <p class="audio-note"><em>Note: Add your recorded narration as audio/evaluation.mp3</em></p>
            </div>

            <h2>5. Success Cases and Failure Modes</h2>
            
            <h3>Evaluation Metrics</h3>
            <p>
                Before examining successes and failures, we need to understand how door detection systems are evaluated:
            </p>

            <div class="highlight-box">
                <h3>üìä Key Metrics</h3>
                <ul>
                    <li><strong>Precision:</strong> Of all detected doors, how many are real? (TP / (TP + FP))</li>
                    <li><strong>Recall:</strong> Of all real doors, how many were detected? (TP / (TP + FN))</li>
                    <li><strong>mAP (mean Average Precision):</strong> Overall detection quality across IoU thresholds</li>
                    <li><strong>Inference Time:</strong> Milliseconds per frame (critical for real-time use)</li>
                    <li><strong>False Positive Rate:</strong> Incorrect detections that confuse users</li>
                </ul>
            </div>

            <h3>Success Cases: When Detection Works Well</h3>

            <h4>1. Standard Wooden Doors in Good Lighting</h4>
            <p>
                Both classical and deep learning methods achieve &gt;95% accuracy on typical wooden doors with:
            </p>
            <ul>
                <li>Clear rectangular shape with visible frame</li>
                <li>Contrasting color from surrounding walls</li>
                <li>Visible handle or doorknob</li>
                <li>Uniform lighting without harsh shadows</li>
                <li>Frontal or near-frontal viewpoint</li>
            </ul>

            <div class="image-container">
                <img src="images/success-wooden-door.jpg" alt="Successful detection of wooden door" class="main-image">
                <p class="image-caption">Figure 12: High-confidence detection (98%) of standard wooden door [Placeholder]</p>
            </div>

            <h4>2. Hallway Scenes with Multiple Doors</h4>
            <p>
                Deep learning models excel at detecting multiple doors simultaneously in corridor environments:
            </p>
            <ul>
                <li>YOLOv8 can detect 5-10 doors in a single frame</li>
                <li>Vanishing point geometry helps classical methods in hallways</li>
                <li>Repeated patterns (door-wall-door) aid recognition</li>
            </ul>

            <div class="image-container">
                <img src="images/success-hallway.jpg" alt="Multiple doors detected in hallway" class="main-image">
                <p class="image-caption">Figure 13: Multi-door detection in hallway scene [Placeholder]</p>
            </div>

            <h4>3. Doors with Strong Visual Features</h4>
            <p>
                Doors with distinctive features are reliably detected:
            </p>
            <ul>
                <li>Bright-colored doors (red, blue) against neutral walls</li>
                <li>Doors with window panels or decorative elements</li>
                <li>Double doors with symmetry</li>
                <li>Doors with prominent signage or room numbers</li>
            </ul>

            <h3>Failure Modes: Common Detection Challenges</h3>

            <h4>1. Glass Doors (Major Challenge)</h4>
            <p>
                Glass doors are the most common failure case, with detection rates dropping to 30-50% 
                <a href="bibliography.html#ref4">[4]</a>.
            </p>

            <div class="warning-box">
                <h4>‚ö†Ô∏è Why Glass Doors Fail</h4>
                <ul>
                    <li><strong>Weak edges:</strong> Transparent glass has minimal contrast with background</li>
                    <li><strong>Reflections:</strong> Mirror-like surfaces reflect surrounding environment</li>
                    <li><strong>See-through:</strong> Background visible through door confuses detectors</li>
                    <li><strong>Lighting dependency:</strong> Only visible in certain lighting conditions</li>
                </ul>
            </div>

            <div class="image-container">
                <img src="images/failure-glass-door.jpg" alt="Failed detection of glass door" class="main-image">
                <p class="image-caption">Figure 14: Glass door missed by detector (only frame detected) [Placeholder]</p>
            </div>

            <p><strong>Mitigation Strategies:</strong></p>
            <ul>
                <li>Use depth sensors to detect planar surfaces (glass still reflects IR)</li>
                <li>Detect door frames and handles instead of door surface</li>
                <li>Train on augmented data with transparency effects</li>
                <li>Multi-frame temporal integration (glass visibility varies with motion)</li>
            </ul>

            <h4>2. Reflective Surfaces and Mirrors</h4>
            <p>
                Mirrors and polished metal doors create false detections by reflecting other doors or door-like objects.
            </p>
            <ul>
                <li><strong>Problem:</strong> Detector sees reflected door and reports two doors</li>
                <li><strong>Frequency:</strong> 10-15% false positive rate in buildings with mirrors</li>
                <li><strong>Solution:</strong> Depth sensing (mirrors show infinite depth), symmetry analysis</li>
            </ul>

            <h4>3. Partial Occlusions</h4>
            <p>
                People, furniture, or carts blocking part of a door reduce detection accuracy to 60-70%.
            </p>

            <div class="image-container">
                <img src="images/failure-occlusion.jpg" alt="Door partially occluded by person" class="main-image">
                <p class="image-caption">Figure 15: Partial occlusion causing missed detection [Placeholder]</p>
            </div>

            <p><strong>Mitigation:</strong></p>
            <ul>
                <li>Train on occluded examples (data augmentation with cutout)</li>
                <li>Use part-based models that detect door components (frame, handle)</li>
                <li>Temporal tracking: if door was visible in previous frames, maintain detection</li>
            </ul>

            <h4>4. Extreme Viewpoints</h4>
            <p>
                Doors viewed from sharp angles (&gt;60¬∞ from frontal) appear distorted and are often missed.
            </p>
            <ul>
                <li><strong>Issue:</strong> Rectangular shape becomes trapezoid or thin sliver</li>
                <li><strong>Impact:</strong> Accuracy drops from 90% (frontal) to 50% (extreme angle)</li>
                <li><strong>Solution:</strong> Train on multi-view data, use 3D pose estimation</li>
            </ul>

            <h4>5. Poor Lighting Conditions</h4>
            <p>
                Low light, backlighting, and harsh shadows degrade performance:
            </p>

            <table style="width: 100%; border-collapse: collapse; margin: 2rem 0;">
                <thead>
                    <tr style="background: var(--primary-color); color: white;">
                        <th style="padding: 1rem; text-align: left;">Lighting Condition</th>
                        <th style="padding: 1rem; text-align: left;">Classical Methods</th>
                        <th style="padding: 1rem; text-align: left;">Deep Learning</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="background: var(--light-bg);">
                        <td style="padding: 1rem;">Good lighting</td>
                        <td style="padding: 1rem;">75%</td>
                        <td style="padding: 1rem;">90%</td>
                    </tr>
                    <tr>
                        <td style="padding: 1rem;">Low light (&lt;50 lux)</td>
                        <td style="padding: 1rem;">45%</td>
                        <td style="padding: 1rem;">70%</td>
                    </tr>
                    <tr style="background: var(--light-bg);">
                        <td style="padding: 1rem;">Backlighting</td>
                        <td style="padding: 1rem;">40%</td>
                        <td style="padding: 1rem;">65%</td>
                    </tr>
                    <tr>
                        <td style="padding: 1rem;">Harsh shadows</td>
                        <td style="padding: 1rem;">55%</td>
                        <td style="padding: 1rem;">75%</td>
                    </tr>
                </tbody>
            </table>

            <h4>6. Door-Like Objects (False Positives)</h4>
            <p>
                Common false positives include:
            </p>
            <ul>
                <li><strong>Windows:</strong> Similar rectangular shape, especially tall windows</li>
                <li><strong>Cabinets:</strong> Vertical rectangles with handles</li>
                <li><strong>Posters/Artwork:</strong> Door images or rectangular frames</li>
                <li><strong>Elevators:</strong> Often correctly detected, but may need separate class</li>
            </ul>

            <div class="info-box">
                <h4>üí° Reducing False Positives</h4>
                <ul>
                    <li>Add negative examples (windows, cabinets) to training data</li>
                    <li>Use context: doors are typically at floor level, not elevated</li>
                    <li>Check for door-specific features: handles at 90-110cm height</li>
                    <li>Depth verification: doors are recessed or protrude from walls</li>
                </ul>
            </div>

            <h3>Real-World Performance Studies from Research Literature</h3>

            <h4>Study 1: DoorNet (Yang et al., 2020)</h4>
            <p>
                Yang et al. evaluated their DoorNet system on 3,500 indoor images from offices, hospitals, and
                residential buildings <a href="bibliography.html#ref4">[4]</a>:
            </p>

            <div class="highlight-box">
                <h3>üìà DoorNet Performance (CVPR 2020)</h3>
                <ul>
                    <li><strong>Overall mAP@0.5:</strong> 89.3%</li>
                    <li><strong>Wooden doors:</strong> 94.7% (best case)</li>
                    <li><strong>Metal doors:</strong> 91.2%</li>
                    <li><strong>Glass doors:</strong> 52.1% (worst case - identified as major challenge)</li>
                    <li><strong>Comparison:</strong> Outperformed Faster R-CNN (82.1%) and YOLOv3 (79.8%)</li>
                    <li><strong>Key finding:</strong> Geometric priors improved accuracy by 12%</li>
                </ul>
            </div>

            <h4>Study 2: RGB-D Fusion for Glass Doors (Xu et al., 2023)</h4>
            <p>
                Xu et al. specifically addressed the glass door challenge using RGB-D fusion with smartphone
                LiDAR sensors <a href="bibliography.html#ref7">[7]</a>:
            </p>

            <div class="highlight-box">
                <h3>üìà Glass Door Detection Results (Sensors 2023)</h3>
                <ul>
                    <li><strong>RGB-only method:</strong> 45% accuracy on glass doors</li>
                    <li><strong>RGB-D fusion method:</strong> 87% accuracy on glass doors (42% improvement!)</li>
                    <li><strong>User study:</strong> 15 blind participants, 40% fewer collisions with glass doors</li>
                    <li><strong>Limitation:</strong> Requires LiDAR-equipped devices (iPhone 12 Pro+)</li>
                    <li><strong>Dataset:</strong> 1,200 glass door images with depth maps (publicly available)</li>
                </ul>
            </div>

            <h4>Study 3: VLM Zero-Shot Performance (Zhang et al., 2024)</h4>
            <p>
                Recent research on Vision-Language Models shows promising results without task-specific training
                <a href="bibliography.html#ref8">[8]</a>:
            </p>

            <div class="highlight-box">
                <h3>üìà GPT-4V Zero-Shot Results (arXiv 2024)</h3>
                <ul>
                    <li><strong>Overall accuracy:</strong> 81% (no training data!)</li>
                    <li><strong>Wooden doors:</strong> 89%</li>
                    <li><strong>Glass doors:</strong> 68% (better than RGB-only CNNs)</li>
                    <li><strong>Novel door types:</strong> 76% (excellent generalization)</li>
                    <li><strong>Inference time:</strong> 2-5 seconds (too slow for real-time)</li>
                    <li><strong>Key advantage:</strong> Natural language output for accessibility</li>
                </ul>
            </div>

            <h3>Improving Robustness</h3>

            <h4>Multi-Frame Voting</h4>
            <p>
                Instead of relying on a single frame, aggregate detections across 5-10 frames:
            </p>
            <div class="code-block">
<pre>
# Multi-frame voting pseudocode
detection_buffer = []  # Store last N frames

for frame in video_stream:
    detections = model.detect(frame)
    detection_buffer.append(detections)
    
    if len(detection_buffer) &gt; 10:
        detection_buffer.pop(0)
    
    # Vote: keep detections that appear in &gt;50% of frames
    stable_detections = vote(detection_buffer, threshold=0.5)
    
    display(stable_detections)
</pre>
            </div>
            <p><strong>Result:</strong> Reduces false positives by 40%, improves glass door detection by 15%</p>

            <h4>Confidence Thresholding</h4>
            <p>
                Adjust detection threshold based on use case:
            </p>
            <ul>
                <li><strong>High recall (0.3 threshold):</strong> Catch all doors, more false positives (navigation)</li>
                <li><strong>High precision (0.7 threshold):</strong> Only confident detections (safety-critical)</li>
                <li><strong>Adaptive:</strong> Lower threshold in sparse environments, raise in cluttered scenes</li>
            </ul>

            <h3>Key Takeaways</h3>
            <ul>
                <li>Standard wooden doors: &gt;90% accuracy (success case)</li>
                <li>Glass doors: 30-50% accuracy (major failure mode)</li>
                <li>Occlusions and extreme viewpoints significantly degrade performance</li>
                <li>Deep learning outperforms classical methods in all conditions</li>
                <li>Multi-frame voting and depth sensing improve robustness</li>
                <li>False positives (windows, cabinets) occur in 8-10% of detections</li>
            </ul>

            <div class="navigation-buttons" style="display: flex; justify-content: space-between; margin-top: 2rem;">
                <a href="learning.html" class="btn">‚Üê Previous: Learning Methods</a>
                <a href="challenges.html" class="btn btn-primary">Next: Challenges ‚Üí</a>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 Nandini - Computer Vision Tutorial Project</p>
    </footer>
</body>
</html>

